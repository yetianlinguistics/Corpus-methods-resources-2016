{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping bbcnews Comments From Posts: 2016-11-09 13:21:10.700000\n",
      "\n",
      "1000 Comments Processed: 2016-11-09 13:21:44.737000\n",
      "2000 Comments Processed: 2016-11-09 13:22:10.972000\n",
      "3000 Comments Processed: 2016-11-09 13:22:31.412000\n",
      "4000 Comments Processed: 2016-11-09 13:22:49.262000\n",
      "5000 Comments Processed: 2016-11-09 13:23:05.409000\n",
      "6000 Comments Processed: 2016-11-09 13:23:23.534000\n",
      "7000 Comments Processed: 2016-11-09 13:23:42.922000\n",
      "8000 Comments Processed: 2016-11-09 13:24:12.064000\n",
      "9000 Comments Processed: 2016-11-09 13:24:29.709000\n",
      "10000 Comments Processed: 2016-11-09 13:24:52.169000\n",
      "11000 Comments Processed: 2016-11-09 13:25:12.178000\n",
      "12000 Comments Processed: 2016-11-09 13:25:26.173000\n",
      "13000 Comments Processed: 2016-11-09 13:25:44.186000\n",
      "14000 Comments Processed: 2016-11-09 13:26:02.595000\n",
      "15000 Comments Processed: 2016-11-09 13:26:16.169000\n",
      "16000 Comments Processed: 2016-11-09 13:26:31.591000\n",
      "17000 Comments Processed: 2016-11-09 13:26:45.194000\n",
      "18000 Comments Processed: 2016-11-09 13:27:01.310000\n",
      "19000 Comments Processed: 2016-11-09 13:27:19.725000\n",
      "20000 Comments Processed: 2016-11-09 13:27:42.991000\n",
      "21000 Comments Processed: 2016-11-09 13:27:59.488000\n",
      "22000 Comments Processed: 2016-11-09 13:28:23.163000\n",
      "23000 Comments Processed: 2016-11-09 13:28:35.269000\n",
      "24000 Comments Processed: 2016-11-09 13:28:46.113000\n",
      "25000 Comments Processed: 2016-11-09 13:28:59.276000\n",
      "26000 Comments Processed: 2016-11-09 13:29:12.813000\n",
      "27000 Comments Processed: 2016-11-09 13:29:27.692000\n",
      "28000 Comments Processed: 2016-11-09 13:29:38.841000\n",
      "29000 Comments Processed: 2016-11-09 13:29:48.877000\n",
      "30000 Comments Processed: 2016-11-09 13:30:03.396000\n",
      "31000 Comments Processed: 2016-11-09 13:30:29.376000\n",
      "32000 Comments Processed: 2016-11-09 13:30:42.178000\n",
      "33000 Comments Processed: 2016-11-09 13:30:57.241000\n",
      "34000 Comments Processed: 2016-11-09 13:31:08.184000\n",
      "35000 Comments Processed: 2016-11-09 13:31:21.265000\n",
      "36000 Comments Processed: 2016-11-09 13:31:30.080000\n",
      "37000 Comments Processed: 2016-11-09 13:31:46.909000\n",
      "38000 Comments Processed: 2016-11-09 13:32:08.989000\n",
      "39000 Comments Processed: 2016-11-09 13:32:25.675000\n",
      "40000 Comments Processed: 2016-11-09 13:32:42.079000"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "\n",
    "app_id = \"583199328536046\"\n",
    "app_secret = \"b2a956f2d320b1d5fb7ad00af52cf421\" # DO NOT SHARE WITH ANYONE!\n",
    "file_id = \"bbcnews\"  \n",
    "\n",
    "access_token = app_id + \"|\" + app_secret\n",
    "\n",
    "delimiter = \"|\" # csv delimiter\n",
    "\n",
    "page_id_list =  [ 'bbcnews']\n",
    "\n",
    "def request_until_succeed(url):\n",
    "    req = urllib2.Request(url)\n",
    "    success = False\n",
    "    while success is False:\n",
    "        try: \n",
    "            response = urllib2.urlopen(req)\n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception, e:\n",
    "            print e\n",
    "            time.sleep(5)\n",
    "\n",
    "            print \"Error for URL %s: %s\" % (url, datetime.datetime.now())\n",
    "            print \"Retrying.\"\n",
    "\n",
    "            if '400' in str(e):\n",
    "                return None;\n",
    "                #return \"Error in this line87654321\"\n",
    "\n",
    "    return response.read()\n",
    "\n",
    "# Needed to write tricky unicode correctly to csv\n",
    "def unicode_normalize(text):\n",
    "    return text.translate({ 0x2018:0x27, 0x2019:0x27, 0x201C:0x22, \n",
    "                            0x201D:0x22, 0xa0:0x20 }).encode('utf-8')\n",
    "\n",
    "def getFacebookCommentFeedData(status_id, access_token, num_comments):\n",
    "\n",
    "    # Construct the URL string\n",
    "        base = \"https://graph.facebook.com/v2.6\"\n",
    "        node = \"/%s/comments\" % status_id \n",
    "        fields = \"?fields=id,message,like_count,created_time,comments,from,attachment\"\n",
    "        parameters = \"&order=chronological&limit=%s&access_token=%s\" % \\\n",
    "                (num_comments, access_token)\n",
    "        url = base + node + fields + parameters\n",
    "\n",
    "        # retrieve data\n",
    "        data = request_until_succeed(url)\n",
    "        if data is None: #\"Error in this line87654321\": #W None\n",
    "            return None\n",
    "        else:   \n",
    "            return json.loads(data)\n",
    "\n",
    "def processFacebookComment(comment, status_id, parent_id = ''):\n",
    "\n",
    "    # The status is now a Python dictionary, so for top-level items,\n",
    "    # we can simply call the key.\n",
    "\n",
    "    # Additionally, some items may not always exist,\n",
    "    # so must check for existence first\n",
    "\n",
    "    comment_id = comment['id']\n",
    "    comment_message = '' if 'message' not in comment else \\\n",
    "            unicode_normalize(comment['message'])\n",
    "    comment_author = unicode_normalize(comment['from']['name'])\n",
    "    comment_likes = 0 if 'like_count' not in comment else \\\n",
    "            comment['like_count']\n",
    "\n",
    "    if 'attachment' in comment:\n",
    "        attach_tag = \"[[%s]]\" % comment['attachment']['type'].upper()\n",
    "        comment_message = attach_tag if comment_message is '' else \\\n",
    "                (comment_message.decode(\"utf-8\") + \" \" + \\\n",
    "                        attach_tag).encode(\"utf-8\")\n",
    "\n",
    "    # Time needs special care since a) it's in UTC and\n",
    "    # b) it's not easy to use in statistical programs.\n",
    "\n",
    "    comment_published = datetime.datetime.strptime(\n",
    "            comment['created_time'],'%Y-%m-%dT%H:%M:%S+0000')\n",
    "    comment_published = comment_published + datetime.timedelta(hours=-5) # EST\n",
    "    comment_published = comment_published.strftime(\n",
    "            '%Y-%m-%d %H:%M:%S') # best time format for spreadsheet programs\n",
    "\n",
    "    # Return a tuple of all processed data\n",
    "\n",
    "    return (comment_id, status_id, parent_id, comment_message, comment_author,\n",
    "            comment_published, comment_likes)\n",
    "\n",
    "def scrapeFacebookPageFeedComments(page_id, access_token):\n",
    "    for file_id in page_id_list:\n",
    "        with open('%s_facebook_comments.csv' % file_id, 'wb') as file:\n",
    "            w = csv.writer(file, delimiter = delimiter)\n",
    "            w.writerow([\"comment_id\", \"status_id\", \"parent_id\", \"comment_message\", \n",
    "                \"comment_author\", \"comment_published\", \"comment_likes\"])\n",
    "    \n",
    "            num_processed = 0   # keep a count on how many we've processed\n",
    "            scrape_starttime = datetime.datetime.now()\n",
    "    \n",
    "            print \"Scraping %s Comments From Posts: %s\\n\" % \\\n",
    "                    (file_id, scrape_starttime)\n",
    "    \n",
    "            with open('%s_facebook_statuses.csv' % file_id, 'rb') as csvfile:  #read the file from \"get fb posts\"\n",
    "                reader = csv.DictReader(csvfile, delimiter = delimiter)\n",
    "    \n",
    "                #reader = [dict(status_id='759985267390294_1158001970921953')]\n",
    "    \n",
    "                for status in reader:\n",
    "                    has_next_page = True\n",
    "    \n",
    "                    comments = getFacebookCommentFeedData(status['status_id'], \n",
    "                            access_token, 100)\n",
    "    \n",
    "                    while has_next_page and comments is not None:\t\t\t\t\n",
    "                        for comment in comments['data']:\n",
    "                            w.writerow(processFacebookComment(comment, \n",
    "                                status['status_id']))\n",
    "    \n",
    "                            if 'comments' in comment:\n",
    "                                has_next_subpage = True\n",
    "    \n",
    "                                subcomments = getFacebookCommentFeedData(\n",
    "                                        comment['id'], access_token, 100)\n",
    "    \n",
    "                                while has_next_subpage:\n",
    "                                    for subcomment in subcomments['data']:\n",
    "                                        # print (processFacebookComment(\n",
    "                                            # subcomment, status['status_id'], \n",
    "                                            # comment['id']))\n",
    "                                        w.writerow(processFacebookComment(\n",
    "                                                subcomment, \n",
    "                                                status['status_id'], \n",
    "                                                comment['id']))\n",
    "    \n",
    "                                        num_processed += 1\n",
    "                                        if num_processed % 1000 == 0:\n",
    "                                            print \"%s Comments Processed: %s\" % \\\n",
    "                                                    (num_processed, \n",
    "                                                        datetime.datetime.now())\n",
    "    \n",
    "                                    if 'paging' in subcomments:\n",
    "                                        if 'next' in subcomments['paging']:\n",
    "                                            subcomments = json.loads(\n",
    "                                                    request_until_succeed(\n",
    "                                                        subcomments['paging']\\\n",
    "                                                                   ['next']))\n",
    "                                        else:\n",
    "                                            has_next_subpage = False\n",
    "                                    else:\n",
    "                                        has_next_subpage = False\n",
    "    \n",
    "                            # output progress occasionally to make sure code is not\n",
    "                            # stalling\n",
    "                            num_processed += 1\n",
    "                            if num_processed % 1000 == 0:\n",
    "                                print \"%s Comments Processed: %s\" % \\\n",
    "                                        (num_processed, datetime.datetime.now())\n",
    "    \n",
    "                        if 'paging' in comments:\t\t\n",
    "                            if 'next' in comments['paging']:\n",
    "                                comments = json.loads(request_until_succeed(\n",
    "                                            comments['paging']['next']))\n",
    "                            else:\n",
    "                                has_next_page = False\n",
    "                        else:\n",
    "                            has_next_page = False\n",
    "    \n",
    "    \n",
    "            print \"\\nDone!\\n%s Comments Processed in %s\" % \\\n",
    "                    (num_processed, datetime.datetime.now() - scrape_starttime)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scrapeFacebookPageFeedComments(file_id, access_token)\n",
    "\n",
    "\n",
    "# The CSV can be opened in all major statistical programs. Have fun! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
